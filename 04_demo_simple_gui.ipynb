{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "e7b2d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funkcije iz 02 (potrebne za vektorizator)\n",
    "def simple_tokenization(review):\n",
    "    tokens = nltk.tokenize.word_tokenize(review)\n",
    "    tokens_without_punctuation = [token for token in tokens if token not in string.punctuation]\n",
    "    return tokens_without_punctuation\n",
    "\n",
    "def short_form_transform(text):\n",
    "    text=re.sub(\"isn't\", 'is not', text)\n",
    "    text=re.sub(\"aren't\", 'are not', text)\n",
    "    text=re.sub(\"he's\", 'he is', text)\n",
    "    text=re.sub(\"wasn't\", 'was not',text)\n",
    "    text=re.sub(\"there's\", 'there is',text)\n",
    "    text=re.sub(\"couldn't\",'could not',text)\n",
    "    text=re.sub(\"can't\", 'can not', text)\n",
    "    text=re.sub(\"won't\", 'will not',text)\n",
    "    text=re.sub(\"they're\", 'they are',text)\n",
    "    text=re.sub(\"she's\", 'she is',text)\n",
    "    text=re.sub(\"wouldn't\", 'would not',text)\n",
    "    text=re.sub(\"haven't\", 'have not',text)\n",
    "    text=re.sub(\"that's\", 'that is',text)\n",
    "    text=re.sub(\"you've\", 'you have',text)\n",
    "    text=re.sub(\"he's\", 'he is',text)\n",
    "    text=re.sub(\"what's\", 'what is',text)\n",
    "    text=re.sub(\"weren't\", 'were not',text)\n",
    "    text=re.sub(\"we're\", 'we are',text)\n",
    "    text=re.sub(\"hasn't\", 'has not',text)\n",
    "    text=re.sub('iâ€™d','i would',text)\n",
    "    text=re.sub(\"you'd\",'you would',text)\n",
    "    text=re.sub(\"shouldn't\",'should not',text)\n",
    "    text=re.sub(\"let's\",'let us',text)\n",
    "    text=re.sub(\"i've\", 'i have', text)\n",
    "    text=re.sub(\"you've\", 'you have', text)\n",
    "    text=re.sub(\"we've\", 'we have', text)\n",
    "    text=re.sub(\"they've\",'they have',text)\n",
    "    text=re.sub(\"you'll\",'you will',text)\n",
    "    text=re.sub(\"i'm\",'i am',text)\n",
    "    text=re.sub(\"we've\",'we have',text)\n",
    "    text=re.sub(\"it's\",'it is',text)\n",
    "    text=re.sub(\"don't\",'do not',text)\n",
    "    text=re.sub(\"doesn't\", 'does not',text)\n",
    "    text=re.sub(\"didn't\", 'did not', text)\n",
    "    text=re.sub(\"hadn't\", 'had not', text)\n",
    "    text=re.sub(\"mightn't\", 'might not', text)\n",
    "    text=re.sub(\"mustn't\", 'must not', text)\n",
    "    text=re.sub(\"it's\",'it is',text)\n",
    "    return text\n",
    "\n",
    "# eliminacija html tagova\n",
    "def strip_html(review):\n",
    "    return re.sub('<[^<]+?>', '', review)\n",
    "\n",
    "# eliminacija url-ova\n",
    "def strip_url(review):\n",
    "    return re.sub(r'http\\S+', '', review)\n",
    "\n",
    "# c.g.i -> cgi, u.s.a -> usa\n",
    "def full_stop_abbrev_elim(review):\n",
    "    pattern = re.compile(r'\\b(?:[a-z]\\.){2,}', re.I)\n",
    "    review = pattern.sub(lambda m: m.group().replace('.',''), review)\n",
    "    return review\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    tokens_without_stop = [token.strip() for token in tokens if token.strip() not in stopwords_list]\n",
    "    return tokens_without_stop\n",
    "\n",
    "def review_preprocessor(text):\n",
    "    text = text.lower()\n",
    "    text = short_form_transform(text)\n",
    "    text = strip_html(text)\n",
    "    text = strip_url(text)\n",
    "    text = full_stop_abbrev_elim(text)\n",
    "    return text\n",
    "\n",
    "def review_tokenizer(stemming, text):\n",
    "    tokens = simple_tokenization(text)\n",
    "    tokens = remove_stop_words(tokens)\n",
    "            \n",
    "    stems = []\n",
    "        \n",
    "    for token in tokens:\n",
    "        \n",
    "        token_pattern = re.compile(r'\\b[^\\W\\d_]+\\b')\n",
    "        if not token_pattern.match(token) or len(token) <= 2:\n",
    "            continue\n",
    "        \n",
    "        stem = stemming.stem(token)\n",
    "        stems.append(stem)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "id": "6f16c825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tkinter as tk\n",
    "from tkinter import ttk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "id": "2813da99",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer_filename = 'vectorizers/count.vect'\n",
    "with open(count_vectorizer_filename, 'rb') as pickle_file:\n",
    "    count_vectorizer = pickle.load(pickle_file) \n",
    "\n",
    "tf_idf_vectorizer_filename = 'vectorizers/tf_idf.vect'\n",
    "with open(tf_idf_vectorizer_filename, 'rb') as pickle_file:\n",
    "    tf_idf_vectorizer = pickle.load(pickle_file)\n",
    "\n",
    "lr_filename = 'models/log_reg.model'\n",
    "with open(lr_filename, 'rb') as pickle_file:\n",
    "    lr_model = pickle.load(pickle_file)\n",
    "    \n",
    "svm_filename = 'models/svm.model'\n",
    "with open(svm_filename, 'rb') as pickle_file:\n",
    "    svm_model = pickle.load(pickle_file)\n",
    "    \n",
    "knn_filename = 'models/knn.model'\n",
    "with open(knn_filename, 'rb') as pickle_file:\n",
    "    knn_model = pickle.load(pickle_file)\n",
    "    \n",
    "vect_dict = {'count': count_vectorizer, 'tf-idf': tf_idf_vectorizer}\n",
    "model_dict = {'lr': lr_model, 'svm': svm_model, 'knn': knn_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "id": "e59dc33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = tk.Tk()\n",
    "root.geometry('250x170')\n",
    "root.title('Sentiment analyzer')\n",
    "root.configure(background='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "id": "d1d369c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_input():\n",
    "    review = inputtxt.get(\"1.0\", \"end-1c\")\n",
    "    \n",
    "    # odabrani vektorizator i model\n",
    "    selected_vectorizer = chosen_vectorizer['values'][chosen_vectorizer.current()]\n",
    "    selected_model = chosen_model['values'][chosen_model.current()]\n",
    "    tk.messagebox.showinfo(title='Information', message=f'Using {selected_vectorizer} for tokenization and {selected_model} model for prediction!')\n",
    "    \n",
    "    \n",
    "    review_vec = vect_dict[selected_vectorizer].transform([review])\n",
    "    predicted_label = model_dict[selected_model].predict(review_vec)[0]\n",
    "    \n",
    "    predicted_proba = model_dict[selected_model].predict_proba(review_vec)[0]\n",
    "      \n",
    "    output.delete('1.0', tk.END)\n",
    "    \n",
    "    if predicted_label == 0:\n",
    "        output.insert('1.0', 'Negative review')\n",
    "    else:\n",
    "        output.insert('1.0', 'Positive review')\n",
    "        \n",
    "    output_proba.delete('1.0', tk.END)\n",
    "    output_proba.insert('1.0', f'Negative prob: {predicted_proba[0]:.5f}\\nPositive prob: {predicted_proba[1]:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "id": "55dfd1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_command():\n",
    "    inputtxt.delete('1.0', tk.END)\n",
    "    output.delete('1.0', tk.END)\n",
    "    output_proba.delete('1.0', tk.END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "id": "e048d1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_l = tk.Label(root, text='Select vectorizer')\n",
    "n = tk.StringVar()\n",
    "chosen_vectorizer = ttk.Combobox(root, width=20, textvariable=n)\n",
    "chosen_vectorizer['values'] = tuple(vect_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "id": "3b466fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l = tk.Label(root, text='Select model')\n",
    "n = tk.StringVar()\n",
    "chosen_model = ttk.Combobox(root, width=20, textvariable=n)\n",
    "chosen_model['values'] = tuple(model_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "id": "4e5e037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = tk.Label(root, text='Insert review:')\n",
    "l.config(font=('Courier', 14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "id": "f0848aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputtxt = tk.Text(root, height=10, width=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "id": "835dae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tk.Text(root, height=2, width=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "id": "fa820b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_proba = tk.Text(root, height=2, width=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "id": "42d721f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_button = tk.Button(root, text='Predict', command=lambda:take_input())\n",
    "clear_button = tk.Button(root, text='Clear', command=lambda:clear_command())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "9a3afe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_l.pack(padx=10, pady=10)\n",
    "chosen_vectorizer.pack()\n",
    "\n",
    "model_l.pack(padx=10, pady=10)\n",
    "chosen_model.pack()\n",
    "\n",
    "l.pack(padx=10, pady=10)\n",
    "inputtxt.pack()\n",
    "\n",
    "predict_button.pack(padx=10, pady=10)\n",
    "output.pack(padx=10, pady=10)\n",
    "output_proba.pack()\n",
    "\n",
    "clear_button.pack()\n",
    "\n",
    "tk.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
